import streamlit as st
import pandas as pd
import json
import os
from datetime import datetime, timedelta

st.set_page_config(
    page_title="å¤§æ¨¡å‹å­¦ä¹ å¹³å°",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å­¦ä¹ è®¡åˆ’æ•°æ®
LEARNING_PLAN = {
    "ç¬¬ä¸€é˜¶æ®µï¼šè®¤çŸ¥ä¸åŸºç¡€ (Week 1-2)": {
        "tasks": [
            {
                "name": "Day 1-2: LLM åŸºç¡€æ¦‚å¿µ",
                "ddl": 2,
                "resources": [
                    {"title": "3Blue1Brown - Attention æœºåˆ¶", "url": "https://www.youtube.com/watch?v=eMlx5fFNoYc"},
                    {"title": "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹", "url": "https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/"},
                    {"title": "LLM å‘å±•å²", "url": "https://huggingface.co/blog/large-language-models"}
                ],
                "exercise": "ç”¨è‡ªå·±çš„è¯è§£é‡Šï¼šTokenizationã€Embeddingã€Attentionã€Transformer å››ä¸ªæ¦‚å¿µ",
                "hint": "æ€è€ƒï¼šä¸ºä»€ä¹ˆ GPT ä¸èƒ½ç›´æ¥ç†è§£æ–‡å­—ï¼ŸToken æ˜¯ä»€ä¹ˆï¼ŸAttention åœ¨åšä»€ä¹ˆè®¡ç®—ï¼Ÿ"
            },
            {
                "name": "Day 3-4: Transformer æ¶æ„æ·±å…¥",
                "ddl": 4,
                "resources": [
                    {"title": "The Illustrated Transformer", "url": "https://jalammar.github.io/illustrated-transformer/"},
                    {"title": "Transformer è®ºæ–‡ç²¾è¯»", "url": "https://www.youtube.com/watch?v=nzqlFIcCSWQ"},
                    {"title": "Let's build GPT (Karpathy)", "url": "https://www.youtube.com/watch?v=kCc8FmEb1nY"}
                ],
                "exercise": "ç»˜åˆ¶ Transformer å®Œæ•´æ¶æ„å›¾ï¼Œæ‰‹åŠ¨è®¡ç®—ä¸€æ¬¡ Self-Attentionï¼ˆ3ä¸ªè¯çš„ä¾‹å­ï¼‰",
                "hint": "Q=WQ*X, K=WK*X, V=WV*X, Attention(Q,K,V) = softmax(QK^T/âˆšd_k)Vï¼Œé‡ç‚¹ç†è§£ Multi-Head"
            },
            {
                "name": "Day 5-6: Prompt Engineering åŸºç¡€",
                "ddl": 6,
                "resources": [
                    {"title": "å´æ©è¾¾ Prompt è¯¾ç¨‹", "url": "https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/"},
                    {"title": "Prompt Engineering Guide", "url": "https://www.promptingguide.ai/zh"},
                    {"title": "OpenAI Prompt æœ€ä½³å®è·µ", "url": "https://platform.openai.com/docs/guides/prompt-engineering"}
                ],
                "exercise": "æŒæ¡ 6 ç§ Prompt æŠ€å·§ï¼šZero-shotã€Few-shotã€CoTã€Self-Consistencyã€ToTã€ReAct",
                "hint": "å®è·µï¼šå†™ä¸€ä¸ªæ—…æ¸¸è§„åˆ’ Promptï¼Œè¦æ±‚è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…å«æ™¯ç‚¹ã€é¢„ç®—ã€æ—¶é—´å®‰æ’"
            },
            {
                "name": "Day 7-8: Prompt è¿›é˜¶æŠ€å·§",
                "ddl": 8,
                "resources": [
                    {"title": "Advanced Prompting", "url": "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"},
                    {"title": "Prompt æ³¨å…¥æ”»é˜²", "url": "https://learnprompting.org/docs/prompt_hacking/injection"}
                ],
                "exercise": "å®ç° 3 ä¸ªè§’è‰² Promptï¼šLinux ç»ˆç«¯ã€Python è§£é‡Šå™¨ã€é¢è¯•å®˜",
                "hint": "ç”¨ System Message å®šä¹‰è§’è‰²ï¼Œç”¨ Few-shot ç¤ºä¾‹çº¦æŸè¾“å‡ºæ ¼å¼"
            },
            {
                "name": "Day 9-10: OpenAI API å®æˆ˜",
                "ddl": 10,
                "resources": [
                    {"title": "OpenAI API æ–‡æ¡£", "url": "https://platform.openai.com/docs/quickstart"},
                    {"title": "API å‚æ•°è¯¦è§£", "url": "https://platform.openai.com/docs/api-reference/chat"},
                    {"title": "Token è®¡è´¹è§„åˆ™", "url": "https://openai.com/pricing"}
                ],
                "exercise": "å®ç°ä¸€ä¸ªå¤šè½®å¯¹è¯ç¿»è¯‘åŠ©æ‰‹ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†ã€æµå¼è¾“å‡ºã€Token ç»Ÿè®¡",
                "hint": "temperatureã€top_pã€max_tokensã€frequency_penalty å‚æ•°çš„ä½œç”¨ï¼Œå¦‚ä½•è®¡ç®—æˆæœ¬"
            },
            {
                "name": "Day 11-12: LangChain æ¡†æ¶å…¥é—¨",
                "ddl": 12,
                "resources": [
                    {"title": "LangChain å¿«é€Ÿå¼€å§‹", "url": "https://python.langchain.com/docs/get_started/quickstart"},
                    {"title": "LangChain æ ¸å¿ƒæ¦‚å¿µ", "url": "https://python.langchain.com/docs/modules/"},
                    {"title": "LCEL è¡¨è¾¾å¼", "url": "https://python.langchain.com/docs/expression_language/"}
                ],
                "exercise": "ç”¨ LangChain å®ç°ï¼šPromptTemplate + LLM + OutputParser çš„å®Œæ•´é“¾è·¯",
                "hint": "æŒæ¡ Chainã€Memoryã€Agent ä¸‰å¤§æ ¸å¿ƒç»„ä»¶"
            },
            {
                "name": "Day 13-14: æ¨¡å‹è¯„ä¼°ä¸æµ‹è¯•",
                "ddl": 14,
                "resources": [
                    {"title": "å¦‚ä½•è¯„ä¼° LLM", "url": "https://huggingface.co/blog/evaluating-llm-chat-models"},
                    {"title": "MMLU/HellaSwag åŸºå‡†", "url": "https://github.com/hendrycks/test"}
                ],
                "exercise": "å¯¹æ¯” GPT-3.5 å’Œ GPT-4 åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚ï¼ˆå‡†ç¡®ç‡ã€é€Ÿåº¦ã€æˆæœ¬ï¼‰",
                "hint": "ä½¿ç”¨ 5-10 ä¸ªæµ‹è¯•æ ·ä¾‹ï¼Œè®°å½•è¾“å‡ºè´¨é‡ã€å“åº”æ—¶é—´ã€Token æ¶ˆè€—"
            }
        ]
    },
    "ç¬¬äºŒé˜¶æ®µï¼šRAG å¼€å‘ (Week 3)": {
        "tasks": [
            {
                "name": "Day 15-16: Embedding ä¸å‘é‡æ£€ç´¢",
                "ddl": 16,
                "resources": [
                    {"title": "Vector Embeddings åŸç†", "url": "https://www.pinecone.io/learn/vector-embeddings/"},
                    {"title": "text-embedding-ada-002", "url": "https://platform.openai.com/docs/guides/embeddings"},
                    {"title": "å‘é‡ç›¸ä¼¼åº¦è®¡ç®—", "url": "https://www.pinecone.io/learn/vector-similarity/"}
                ],
                "exercise": "ç†è§£ Cosine Similarityã€Euclidean Distanceã€Dot Product çš„åŒºåˆ«ï¼Œæ‰‹åŠ¨è®¡ç®—ç¤ºä¾‹",
                "hint": "ä¸ºä»€ä¹ˆ Embedding èƒ½æ•æ‰è¯­ä¹‰ï¼Ÿ768 ç»´å‘é‡ä»£è¡¨ä»€ä¹ˆï¼Ÿå½’ä¸€åŒ–çš„ä½œç”¨ï¼Ÿ"
            },
            {
                "name": "Day 17-18: ChromaDB å®æˆ˜",
                "ddl": 18,
                "resources": [
                    {"title": "ChromaDB å¿«é€Ÿå¼€å§‹", "url": "https://docs.trychroma.com/getting-started"},
                    {"title": "å‘é‡æ•°æ®åº“å¯¹æ¯”", "url": "https://github.com/qdrant/vector-db-benchmark"}
                ],
                "exercise": "å®ç°æ–‡æ¡£åˆ‡ç‰‡ â†’ Embedding â†’ å­˜å‚¨ â†’ è¯­ä¹‰æœç´¢å®Œæ•´æµç¨‹",
                "hint": "RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
            },
            {
                "name": "Day 19-20: RAG æ ¸å¿ƒæµç¨‹",
                "ddl": 20,
                "resources": [
                    {"title": "LangChain RAG", "url": "https://python.langchain.com/docs/use_cases/question_answering/"},
                    {"title": "RAG è®ºæ–‡", "url": "https://arxiv.org/abs/2005.11401"}
                ],
                "exercise": "æ„å»ºä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼ˆæ”¯æŒ PDF/Markdown å¯¼å…¥ï¼‰",
                "hint": "Retriever â†’ Prompt â†’ LLM â†’ Answerï¼Œæ³¨æ„ Context é•¿åº¦æ§åˆ¶"
            },
            {
                "name": "Day 21: è¿›é˜¶ RAG ä¼˜åŒ–",
                "ddl": 21,
                "resources": [
                    {"title": "Advanced RAG", "url": "https://www.pinecone.io/learn/advanced-rag/"},
                    {"title": "Reranking æŠ€æœ¯", "url": "https://www.sbert.net/examples/applications/cross-encoder/README.html"}
                ],
                "exercise": "å®ç° Hybrid Searchï¼ˆBM25 + Vectorï¼‰+ Reranking + å¼•ç”¨æ¥æºæ ‡æ³¨",
                "hint": "æ£€ç´¢ Top-20 â†’ Rerank â†’ å– Top-5 â†’ æ³¨å…¥ Promptï¼Œè¾“å‡º [æ¥æº1][æ¥æº2]"
            }
        ]
    },
    "ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹å¾®è°ƒ (Week 4-7)": {
        "tasks": [
            {
                "name": "Day 15-17: å¾®è°ƒç†è®ºåŸºç¡€",
                "ddl": 17,
                "resources": [
                    {"title": "Fine-tuning åŸç†", "url": "https://huggingface.co/blog/fine-tune-llms"},
                    {"title": "LoRA è®ºæ–‡è§£è¯»", "url": "https://arxiv.org/abs/2106.09685"},
                    {"title": "PEFT æ–‡æ¡£", "url": "https://huggingface.co/docs/peft/index"}
                ],
                "exercise": "ç†è§£ 4 ç§å¾®è°ƒæ–¹æ³•ï¼šFull Fine-tuningã€Adapterã€Prefix Tuningã€LoRA çš„åŒºåˆ«",
                "hint": "å¯¹æ¯”å‚æ•°é‡ã€æ˜¾å­˜å ç”¨ã€è®­ç»ƒé€Ÿåº¦ã€æ•ˆæœã€‚ä¸ºä»€ä¹ˆ LoRA åªè®­ç»ƒ 0.1% å‚æ•°å´æ•ˆæœå¥½ï¼Ÿ"
            },
            {
                "name": "Day 18-20: ç¯å¢ƒæ­å»ºä¸æ¨¡å‹åŠ è½½",
                "ddl": 20,
                "resources": [
                    {"title": "Transformers å¿«é€Ÿå¼€å§‹", "url": "https://huggingface.co/docs/transformers/quicktour"},
                    {"title": "æ¨¡å‹é‡åŒ– (4bit/8bit)", "url": "https://huggingface.co/blog/4bit-transformers-bitsandbytes"},
                    {"title": "Accelerate åº“", "url": "https://huggingface.co/docs/accelerate/index"}
                ],
                "exercise": "åœ¨ Colab (T4 GPU) åŠ è½½ Qwen-7B-Chatï¼Œå®ç° 4bit é‡åŒ–æ¨ç†",
                "hint": "ä½¿ç”¨ BitsAndBytesConfig + load_in_4bit=True èŠ‚çœæ˜¾å­˜ï¼Œfrom_pretrained å‚æ•°è¯¦è§£"
            },
            {
                "name": "Day 21-23: æ•°æ®é›†æ„å»ºä¸å¤„ç†",
                "ddl": 23,
                "resources": [
                    {"title": "Alpaca æ•°æ®é›†", "url": "https://github.com/tatsu-lab/stanford_alpaca"},
                    {"title": "æ•°æ®æ ¼å¼è§„èŒƒ", "url": "https://huggingface.co/docs/datasets/about_dataset_load"},
                    {"title": "Tokenization æŠ€å·§", "url": "https://huggingface.co/docs/transformers/preprocessing"}
                ],
                "exercise": "æ„å»º 100 æ¡é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ˆé€‰æ‹©ä¸€ä¸ªå‚ç›´é¢†åŸŸï¼šåŒ»ç–—/æ³•å¾‹/ç¼–ç¨‹/å®¢æœï¼‰",
                "hint": "æ ¼å¼ï¼š{instruction, input, output}ã€‚ç¡®ä¿å¤šæ ·æ€§ï¼šé—®ç­”ã€æ€»ç»“ã€ç¿»è¯‘ã€ç”Ÿæˆç­‰"
            },
            {
                "name": "Day 24-26: LoRA å¾®è°ƒå®æˆ˜",
                "ddl": 26,
                "resources": [
                    {"title": "LoRA å®˜æ–¹ä»£ç ", "url": "https://github.com/microsoft/LoRA"},
                    {"title": "PEFT + Transformers", "url": "https://huggingface.co/blog/peft"},
                    {"title": "è®­ç»ƒå‚æ•°è°ƒä¼˜", "url": "https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2"}
                ],
                "exercise": "ä½¿ç”¨ LoRA å¾®è°ƒ Qwen-7Bï¼Œå®ç°ç‰¹å®šé£æ ¼è¾“å‡ºï¼ˆä¾‹å¦‚ï¼šçŒ«å¨˜ã€å¤é£ã€æŠ€æœ¯åšä¸»ï¼‰",
                "hint": "é‡ç‚¹å‚æ•°ï¼šr=8, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj','v_proj']"
            },
            {
                "name": "Day 27-29: QLoRA ä¸æ˜¾å­˜ä¼˜åŒ–",
                "ddl": 29,
                "resources": [
                    {"title": "QLoRA è®ºæ–‡", "url": "https://arxiv.org/abs/2305.14314"},
                    {"title": "Gradient Checkpointing", "url": "https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing"},
                    {"title": "æ˜¾å­˜ä¼˜åŒ–æŠ€å·§", "url": "https://huggingface.co/docs/transformers/perf_train_gpu_one"}
                ],
                "exercise": "ç”¨ QLoRA åœ¨ 12GB æ˜¾å¡ä¸Šå¾®è°ƒ 13B æ¨¡å‹ï¼ˆå¯¹æ¯” LoRA çš„æ˜¾å­˜å ç”¨ï¼‰",
                "hint": "4bit é‡åŒ– + NF4 æ•°æ®ç±»å‹ + double quantizationï¼Œbatch_size=1, gradient_accumulation_steps=4"
            },
            {
                "name": "Day 30-32: LLaMA-Factory å…¨æµç¨‹",
                "ddl": 32,
                "resources": [
                    {"title": "LLaMA-Factory", "url": "https://github.com/hiyouga/LLaMA-Factory"},
                    {"title": "WebUI ä½¿ç”¨æ•™ç¨‹", "url": "https://www.youtube.com/watch?v=your-tutorial"},
                    {"title": "é…ç½®æ–‡ä»¶è¯¦è§£", "url": "https://github.com/hiyouga/LLaMA-Factory/wiki"}
                ],
                "exercise": "ç”¨ LLaMA-Factory å®Œæˆï¼šæ•°æ®å‡†å¤‡ â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ å¯¼å‡º â†’ éƒ¨ç½²å®Œæ•´æµç¨‹",
                "hint": "llamafactory-cli train --stage sft --model_name_or_path qwen --dataset alpaca_zh"
            },
            {
                "name": "Day 33-35: å…¨å‚æ•°å¾®è°ƒ (SFT)",
                "ddl": 35,
                "resources": [
                    {"title": "Supervised Fine-Tuning", "url": "https://huggingface.co/blog/llama2#how-to-prompt-llama-2"},
                    {"title": "DeepSpeed ZeRO", "url": "https://www.deepspeed.ai/tutorials/zero/"},
                    {"title": "FSDP åˆ†å¸ƒå¼è®­ç»ƒ", "url": "https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/"}
                ],
                "exercise": "ç†è§£å…¨å‚æ•°å¾®è°ƒ vs LoRA çš„é€‚ç”¨åœºæ™¯ï¼Œä»€ä¹ˆæ—¶å€™å¿…é¡»ç”¨å…¨å‚æ•°ï¼Ÿ",
                "hint": "é¢†åŸŸçŸ¥è¯†æ³¨å…¥ã€è¯­è¨€è¿ç§»éœ€è¦å…¨å‚æ•°ï¼›é£æ ¼è°ƒæ•´ã€ä»»åŠ¡é€‚é…ç”¨ LoRA"
            },
            {
                "name": "Day 36-38: RLHF ä¸ DPO",
                "ddl": 38,
                "resources": [
                    {"title": "RLHF åŸç†", "url": "https://huggingface.co/blog/rlhf"},
                    {"title": "DPO è®ºæ–‡", "url": "https://arxiv.org/abs/2305.18290"},
                    {"title": "TRL åº“", "url": "https://github.com/huggingface/trl"}
                ],
                "exercise": "æ„å»ºåå¥½æ•°æ®é›†ï¼ˆchosen vs rejectedï¼‰ï¼Œç†è§£ PPO è®­ç»ƒæµç¨‹",
                "hint": "RLHF ä¸‰é˜¶æ®µï¼šSFT â†’ Reward Model â†’ PPOã€‚DPO ç›´æ¥ä¼˜åŒ–ï¼Œæ— éœ€ RM"
            },
            {
                "name": "Day 39-42: æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²",
                "ddl": 42,
                "resources": [
                    {"title": "æ¨¡å‹è¯„ä¼°æŒ‡æ ‡", "url": "https://huggingface.co/spaces/evaluate-metric/perplexity"},
                    {"title": "vLLM é«˜æ€§èƒ½æ¨ç†", "url": "https://github.com/vllm-project/vllm"},
                    {"title": "æ¨¡å‹é‡åŒ–éƒ¨ç½²", "url": "https://github.com/ggerganov/llama.cpp"}
                ],
                "exercise": "è¯„ä¼°å¾®è°ƒåæ¨¡å‹ï¼šPPLã€BLEUã€äººå·¥è¯„åˆ†ï¼Œå¯¹æ¯”å¾®è°ƒå‰åå·®å¼‚",
                "hint": "ä½¿ç”¨ vLLM éƒ¨ç½²ï¼Œå¯¹æ¯”æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰ã€æ˜¾å­˜å ç”¨ã€å¹¶å‘èƒ½åŠ›"
            },
            {
                "name": "Day 43-45: æŒç»­å­¦ä¹ ä¸ç¾éš¾é—å¿˜",
                "ddl": 45,
                "resources": [
                    {"title": "Catastrophic Forgetting", "url": "https://arxiv.org/abs/2002.06305"},
                    {"title": "Elastic Weight Consolidation", "url": "https://arxiv.org/abs/1612.00796"}
                ],
                "exercise": "å¾®è°ƒåæµ‹è¯•é€šç”¨èƒ½åŠ›æ˜¯å¦ä¸‹é™ï¼ˆåŠ æ³•è¿ç®—ã€å¸¸è¯†é—®ç­”ï¼‰ï¼Œå¦‚ä½•ç¼“è§£ï¼Ÿ",
                "hint": "æ··åˆé€šç”¨æ•°æ®é›†ã€æ§åˆ¶å­¦ä¹ ç‡ã€ä½¿ç”¨ EWC æ­£åˆ™åŒ–"
            }
        ]
    },
    "ç¬¬å››é˜¶æ®µï¼šAgent å¼€å‘ (Week 8)": {
        "tasks": [
            {
                "name": "Day 46-48: Agent åŸºç¡€ä¸ ReAct",
                "ddl": 48,
                "resources": [
                    {"title": "ReAct è®ºæ–‡", "url": "https://arxiv.org/abs/2210.03629"},
                    {"title": "LangChain Agent", "url": "https://python.langchain.com/docs/modules/agents/"},
                    {"title": "Agent è®¾è®¡æ¨¡å¼", "url": "https://lilianweng.github.io/posts/2023-06-23-agent/"}
                ],
                "exercise": "å®ç° ReAct Agentï¼šQuestion â†’ Thought â†’ Action â†’ Observation å¾ªç¯",
                "hint": "å·¥å…·ï¼šCalculatorã€Wikipediaã€Weather APIï¼Œæœ€å¤š 5 è½®å¾ªç¯"
            },
            {
                "name": "Day 49-51: Function Calling",
                "ddl": 51,
                "resources": [
                    {"title": "OpenAI Function Calling", "url": "https://platform.openai.com/docs/guides/function-calling"},
                    {"title": "å·¥å…·å®šä¹‰è§„èŒƒ", "url": "https://json-schema.org/"}
                ],
                "exercise": "æ„å»ºæ™ºèƒ½åŠ©æ‰‹ï¼šå¤©æ°”æŸ¥è¯¢ + æ—¥å†ç®¡ç† + é‚®ä»¶å‘é€ï¼ˆ3 ä¸ª Functionï¼‰",
                "hint": "å®šä¹‰ JSON Schema â†’ æ¨¡å‹è¿”å› function_call â†’ æ‰§è¡Œå‡½æ•° â†’ è¿”å›ç»“æœ"
            },
            {
                "name": "Day 52-54: å¤š Agent åä½œ",
                "ddl": 54,
                "resources": [
                    {"title": "AutoGen", "url": "https://github.com/microsoft/autogen"},
                    {"title": "MetaGPT", "url": "https://github.com/geekan/MetaGPT"},
                    {"title": "CrewAI", "url": "https://github.com/joaomdmoura/crewAI"}
                ],
                "exercise": "å®ç°åŒ Agent Code Reviewï¼šCoder (å†™ä»£ç ) + Reviewer (å®¡æŸ¥ä»£ç )",
                "hint": "UserProxy â†” Assistantï¼Œæœ€å¤š 3 è½®å¯¹è¯è¾¾æˆä¸€è‡´"
            },
            {
                "name": "Day 55-56: Memory ä¸ä¸Šä¸‹æ–‡ç®¡ç†",
                "ddl": 56,
                "resources": [
                    {"title": "LangChain Memory", "url": "https://python.langchain.com/docs/modules/memory/"},
                    {"title": "ä¸Šä¸‹æ–‡çª—å£ä¼˜åŒ–", "url": "https://github.com/hwchase17/chat-langchain"}
                ],
                "exercise": "å®ç° ConversationBufferMemoryã€ConversationSummaryMemory å¹¶å¯¹æ¯”",
                "hint": "è¶…è¿‡ 4k tokens å¦‚ä½•å‹ç¼©ï¼Ÿå¦‚ä½•ä¿ç•™å…³é”®ä¿¡æ¯ï¼Ÿ"
            },
            {
                "name": "Day 57-60: å®Œæ•´é¡¹ç›®å®æˆ˜",
                "ddl": 60,
                "resources": [
                    {"title": "LangChain é¡¹ç›®æ¡ˆä¾‹", "url": "https://github.com/langchain-ai/langchain/tree/master/templates"},
                    {"title": "Streamlit éƒ¨ç½²", "url": "https://docs.streamlit.io/streamlit-community-cloud"}
                ],
                "exercise": "ç»¼åˆé¡¹ç›®ï¼šåŸºäº RAG + Agent çš„æ™ºèƒ½å®¢æœç³»ç»Ÿï¼ˆçŸ¥è¯†åº“æ£€ç´¢ + å·¥å…·è°ƒç”¨ + å¤šè½®å¯¹è¯ï¼‰",
                "hint": "æ•´åˆæ‰€æœ‰çŸ¥è¯†ç‚¹ï¼Œéƒ¨ç½²åˆ° Streamlit Cloudï¼Œå‡†å¤‡ä½œå“é›†å±•ç¤º"
            }
        ]
    }
}

# åˆå§‹åŒ–è¿›åº¦æ–‡ä»¶
PROGRESS_FILE = "data/progress.json"

def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_progress(progress):
    os.makedirs('data', exist_ok=True)
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)

# ä¾§è¾¹æ 
st.sidebar.title("ğŸš€ LLM å­¦ä¹ å¹³å°")
st.sidebar.markdown("---")

# å¼€å§‹æ—¥æœŸè®¾ç½®
if 'start_date' not in st.session_state:
    st.session_state.start_date = datetime.now()

start_date = st.sidebar.date_input(
    "è®¾ç½®å­¦ä¹ å¼€å§‹æ—¥æœŸ",
    value=st.session_state.start_date
)
st.session_state.start_date = datetime.combine(start_date, datetime.min.time())

# è®¡ç®—å½“å‰å¤©æ•°
current_day = (datetime.now() - st.session_state.start_date).days + 1
st.sidebar.metric("å­¦ä¹ è¿›åº¦", f"ç¬¬ {current_day} å¤©", "å…± 60 å¤©")

# åŠ è½½è¿›åº¦
progress = load_progress()

# ä¸»é¡µé¢
st.title("ğŸ§  å¤§æ¨¡å‹ (LLM) ä»0åˆ°1 å­¦ä¹ å¹³å°")
st.markdown("**æ ¸å¿ƒç†å¿µ**: Project-based Learning (PBL) - è¾¹å­¦è¾¹ç»ƒ")

# æ˜¾ç¤ºé˜¶æ®µ
tabs = st.tabs(list(LEARNING_PLAN.keys()))

for tab_idx, (stage_name, stage_data) in enumerate(LEARNING_PLAN.items()):
    with tabs[tab_idx]:
        st.header(stage_name)
        
        for task_idx, task in enumerate(stage_data["tasks"]):
            task_id = f"{tab_idx}_{task_idx}"
            task_status = progress.get(task_id, {"completed": False, "notes": ""})
            
            with st.expander(f"ğŸ“Œ {task['name']} (DDL: Day {task['ddl']})", expanded=True):
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.markdown("**ğŸ“š å­¦ä¹ èµ„æº**:")
                    for res in task["resources"]:
                        st.markdown(f"- [{res['title']}]({res['url']})")
                    
                    st.markdown(f"**âœï¸ å®æˆ˜ç»ƒä¹ **: {task['exercise']}")
                    st.info(f"ğŸ’¡ æç¤º: {task['hint']}")
                    
                    # ç¬”è®°åŒº
                    notes = st.text_area(
                        "ğŸ“ å­¦ä¹ ç¬”è®°",
                        value=task_status.get("notes", ""),
                        key=f"notes_{task_id}",
                        height=100
                    )
                    
                    if st.button(f"ğŸ’¾ ä¿å­˜ç¬”è®°", key=f"save_{task_id}"):
                        progress[task_id] = progress.get(task_id, {})
                        progress[task_id]["notes"] = notes
                        save_progress(progress)
                        st.success("âœ… ç¬”è®°å·²ä¿å­˜")
                        st.rerun()
                
                with col2:
                    # DDL å€’è®¡æ—¶
                    ddl_date = st.session_state.start_date + timedelta(days=task['ddl'])
                    days_left = (ddl_date - datetime.now()).days
                    
                    if days_left > 0:
                        st.metric("â° å‰©ä½™", f"{days_left} å¤©")
                    elif days_left == 0:
                        st.warning("âš ï¸ ä»Šå¤©æˆªæ­¢")
                    else:
                        st.error(f"âŒ å·²è¶…æœŸ {abs(days_left)} å¤©")
                    
                    # å®ŒæˆçŠ¶æ€
                    checkbox_key = f"complete_{task_id}"
                    completed = st.checkbox(
                        "âœ… å·²å®Œæˆ",
                        value=task_status.get("completed", False),
                        key=checkbox_key
                    )
                    
                    # æ£€æµ‹çŠ¶æ€å˜åŒ–å¹¶ä¿å­˜
                    if checkbox_key in st.session_state:
                        current_value = st.session_state[checkbox_key]
                        if current_value != task_status.get("completed", False):
                            progress[task_id] = progress.get(task_id, {})
                            progress[task_id]["completed"] = current_value
                            progress[task_id]["completed_date"] = datetime.now().strftime("%Y-%m-%d") if current_value else None
                            save_progress(progress)
                            if current_value:
                                st.balloons()

# åº•éƒ¨ç»Ÿè®¡
st.markdown("---")
st.subheader("ğŸ“Š å­¦ä¹ ç»Ÿè®¡")

col1, col2, col3 = st.columns(3)

total_tasks = sum(len(stage["tasks"]) for stage in LEARNING_PLAN.values())
completed_tasks = sum(1 for task in progress.values() if task.get("completed"))

with col1:
    st.metric("æ€»ä»»åŠ¡æ•°", total_tasks)
with col2:
    st.metric("å·²å®Œæˆ", completed_tasks)
with col3:
    completion_rate = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
    st.metric("å®Œæˆç‡", f"{completion_rate:.1f}%")

# è¿›åº¦æ¡
st.progress(completion_rate / 100)
