"""
Week 2 - Day 14: ä¸ªäººçŸ¥è¯†åº“é—®ç­”æœºå™¨äºº (RAG)
ç»ƒä¹ ç›®æ ‡: æŒæ¡ RAG å…¨æµç¨‹
"""

from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv

load_dotenv()

class KnowledgeBaseQA:
    """ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, persist_directory="./data/chroma_db"):
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        self.qa_chain = None
    
    def load_documents(self, file_paths):
        """åŠ è½½æ–‡æ¡£"""
        documents = []
        for path in file_paths:
            if path.endswith('.pdf'):
                loader = PyPDFLoader(path)
            else:
                loader = TextLoader(path, encoding='utf-8')
            documents.extend(loader.load())
        return documents
    
    def create_vectorstore(self, documents):
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        # æ–‡æ¡£åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        splits = text_splitter.split_documents(documents)
        
        # å­˜å…¥ ChromaDB
        self.vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        self.vectorstore.persist()
        print(f"âœ… å·²å¤„ç† {len(splits)} ä¸ªæ–‡æ¡£å—")
    
    def build_qa_chain(self):
        """æ„å»ºé—®ç­”é“¾"""
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        
        # è‡ªå®šä¹‰ Prompt
        template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ç­”æ¡ˆ,å°±è¯´ä¸çŸ¥é“,ä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡: {context}

é—®é¢˜: {question}

å›ç­”:"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    def query(self, question):
        """æŸ¥è¯¢é—®ç­”"""
        if not self.qa_chain:
            raise ValueError("è¯·å…ˆè°ƒç”¨ build_qa_chain()")
        
        result = self.qa_chain({"query": question})
        return {
            "answer": result["result"],
            "sources": result["source_documents"]
        }

def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    # åˆå§‹åŒ–
    kb = KnowledgeBaseQA()
    
    # åŠ è½½ä½ çš„ç¬”è®°æ–‡ä»¶ (æ”¯æŒ .txt, .pdf)
    documents = kb.load_documents([
        "data/my_notes.txt",  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
    ])
    
    # æ„å»ºå‘é‡åº“
    kb.create_vectorstore(documents)
    kb.build_qa_chain()
    
    # é—®ç­”
    print("ğŸ’¬ çŸ¥è¯†åº“é—®ç­” (è¾“å…¥ 'quit' é€€å‡º)")
    print("-" * 50)
    
    while True:
        question = input("\nâ“ è¯·æé—®: ").strip()
        if question.lower() in ['quit', 'exit', 'q']:
            break
        
        result = kb.query(question)
        print(f"\nğŸ’¡ å›ç­”: {result['answer']}")
        print(f"\nğŸ“š å¼•ç”¨æ¥æº:")
        for i, doc in enumerate(result['sources'], 1):
            print(f"  [{i}] {doc.metadata.get('source', 'Unknown')}")

if __name__ == "__main__":
    main()

# è¿è¡Œ: python exercises/week2_rag.py
